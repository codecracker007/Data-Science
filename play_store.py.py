# -*- coding: utf-8 -*-
"""Copy of PlayStore_reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EySIO6PJZn57PxoarnQlOEmcykFHNJVo
"""

import os
from google.colab import files
texts =files.upload()

#import pandas and numpy
import pandas as pd
import numpy as np
df = pd.read_csv(r"googleplaystore1.csv")

import seaborn as sns
import matplotlib.pyplot as plot
from datetime import datetime, date

#display the dataset
#before data cleaning
df

"""Data Cleaning"""

df=df.replace(['Varies with device'],np.NaN)

#we drop unecessary columns
df=df.drop(['Category'],axis=1)
df=df.drop(['Current Ver'],axis=1)

#shows the total number of missing values
df.isnull().sum()

mean1= df['Rating'].mean()
mean1

#replace the null values with the mean of the column
df['Rating']=df['Rating'].replace(np.NaN,mean1)

mode2= df['Android Ver'].mode()
mode2

#we replace some columns with their mode due their characteristics
df['Android Ver']=df['Android Ver'].replace(np.NaN,'4.1 and up')

#remove unwanted parts of a string in a column
df['Size'] = df['Size'].str.replace(r'\D', '')
df['Installs'] = df['Installs'].str.replace(r'\D', '')
df['Price'] = df['Price'].str.replace(r'\D', '')

mode1= df['Size'].mode()
mode1

#we replace some columns with their mode due their characteristics
df['Size']=df['Size'].replace(np.NaN,14)

#convert the columns to integer values
df['Size'] = df['Size'].astype(int)
df['Installs'] = df['Installs'].astype(int)
df['Price'] = df['Price'].astype(int)

meanR= df['Reviews'].mean()
meanR

st = df['Rating'].std()
st

df.mean()

from datetime import datetime
for i in range(0,1560):
  df.at[i,'Last Updated'] = datetime.strptime(df.at[i,'Last Updated'], '%B %d, %Y')
df['Last Updated']

#after data cleaning
df

#Data Analysing 
df['Genres'].value_counts()

df['Content Rating'].value_counts()

df['Installs'].value_counts()

df['Type'].value_counts()

"""Graph Visualisation"""

genres=('Communication','Education','Medical','Business','Finance','Lifestyle')
data1=df.loc[df['Genres'].isin(genres)]
sns.countplot(x='Genres',data=data1)

content=('Everyone','Teen','Mature 17+','Everyone')
data2=df.loc[df['Content Rating'].isin(content)]
sns.countplot(x='Content Rating',data=data2)

x=df['Rating']
plot.figure(figsize=(15,8))
ax=sns.distplot(x,bins=58,kde=False, color='r')
ax.set_xlabel(xlabel='App Ratings',fontsize = 16)
plot.show

genres=('Communication','Education','Medical','Business','Finance','Lifestyle')
genres1=('Everyone','Teen','Mature 17+','Everyone 10+','Adults only 18+','Unrated')
d1=df.loc[df['Genres'].isin(genres) & df['Rating']]
plot.rcParams['figure.figsize']=(15,6)
ax= sns.boxplot(x=d1['Genres'],y=d1['Rating'])
ax.set_xlabel(xlabel='Genres')
ax.set_ylabel(ylabel='count')
#plot.xticks(rotation=90)
plot.show()

#the boxplot shows some outliers. But since the data is completely random we cannot expect the data to lie inside the interquartile range

"""Normalisation And Standardisation"""

#we standardise the data here
#standardiser is used here to scale down the data for easy analysation
df_std=df.copy()
for i in df_std.columns[1:5]:
  df_std[i]=(df_std[i]-df_std[i].mean())/df_std[i].std()
df_std.head()

for i in df_std.columns[6:7]:
  df_std[i]=(df_std[i]-df_std[i].mean())/df_std[i].std()
df_std.head()

#normalisation
#convert mean to 0
df_std.mean().round(decimals=1)

#convert std to 1
df_std.var()

from sklearn import preprocessing
x_array = np.array(df['Rating'])
normalized_X = preprocessing.normalize([x_array])
normalized_X

x=normalized_X
plot.figure(figsize=(15,8))
ax=sns.distplot(x,bins=58,kde=False, color='r')
ax.set_xlabel(xlabel='App Ratings',fontsize = 16)
plot.show

"""Hypothesis Testing"""

#Its the process in which the analyst tests an assumption regarding a population parameter.
from scipy.stats import norm
from math import sqrt
def one_sided_hypo(sample_mean,pop_mean,std_dev,sample_size,alpha):
  actual_z=abs(norm.ppf(alpha))
  hypo_z=(sample_mean-pop_mean)/(std_dev/sqrt(sample_size))
  print("actual z value :",actual_z)
  print("hypothesis z value :",hypo_z,'\n')
  if hypo_z >= actual_z:
    return True
  else:
    return False

#we predict that the medical apps have a rating of less than 4.5
pop_mean = 4.5
alpha = 0.05
sample_size=170
print('H0 : u >=',pop_mean)
print('H1 : u <',pop_mean)
print('alpha value is :',alpha,'\n')
reject = one_sided_hypo(mean1,pop_mean,st,sample_size,alpha)
if (reject == True):
  print("Null hypothesis is accepted")
else:
  print("Null hypothesis is Rejected")

#from the above results, we can conclude that the null hypothesis is rejected and so we take the alternate hypothesis
#Hence, medical apps have a rating of less than 4.5

"""Correlation"""

#Correlation is a statistical measure that expresses the extent to which 2 variables are linearly related,i.e they can change together at a constant rate.   
df.corr()

np.corrcoef(df['Reviews'],df['Installs'],rowvar=True)
#Highest Correlation

np.corrcoef(df['Size'],df['Installs'],rowvar=True)
#Lowest Correlation

#positive scattterlot
#a positive correlation, this occurs when value of both the variable increases as they depend on the others
sns.lmplot(x='Reviews', y='Installs',data=df.sample(500))
plot.show()

#negative scatterplot
#a negative correlation, this occurs when value of 1 variable increases and the other decreases as depending on the others.
sns.lmplot(x='Size', y='Installs',data=df)
plot.show()

from google.colab import files
df.to_csv('app.csv') 
files.download('app.csv')

